{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBTIX35bA4BZ",
    "outputId": "2937d0f5-8620-46c3-d240-881bfbdcb77d"
   },
   "outputs": [],
   "source": [
    "!pip3 install tensorflow==2.3.0\n",
    "!pip install gym\n",
    "!pip install keras\n",
    "!pip install keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDvFM8BaA7J6"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "from gym.utils.env_checker import check_env\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2JDvIh3Yjoa",
    "outputId": "1171f959-ebc2-4cfc-d178-ba4637fadb4e"
   },
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if len(device_name) > 0:\n",
    "    print(\"Found GPU at: {}\".format(device_name))\n",
    "else:\n",
    "    device_name = \"/device:CPU:0\"\n",
    "    print(\"No GPU, using {}.\".format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEbwRCbxBibC"
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "\n",
    "STARTING_BUDGET = np.array([10.0], dtype=np.float32)\n",
    "LABOR = 50\n",
    "ALPHA = 0.25 # capital elasticity\n",
    "DELTA = 0.25 # capital depreciation rate\n",
    "EPISODE_LENGTH = 10000\n",
    "BUFFER_CAPACITY = 50000\n",
    "SHOCKS = False\n",
    "SHOCK_TYPE = 'PERSISTENT'\n",
    "DUMMY_LAG = 1 # not used for simulation; only used to initialize dummy environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTHqCWfLBjl3"
   },
   "outputs": [],
   "source": [
    "class CustomEnv(Env):\n",
    "    def __init__(self, lag):\n",
    "        self.action_space = Box(low=0.0, high=1.0, dtype=np.float32)\n",
    "        self.observation_space = Box(low=0.0, high=10000000.0, dtype=np.float32)\n",
    "\n",
    "        self.capital = STARTING_BUDGET # the amount of capital in agent's budget\n",
    "        self.capital_history = []\n",
    "        self.t = 0\n",
    "        self.lag = lag\n",
    "        self.shock = 1\n",
    "    \n",
    "    def step(self, action):\n",
    "        # action: float between 0.0 and 1.0, representing the savings rate for the agent\n",
    "        assert(action >= 0.0 and action <= 1.0)\n",
    "        assert(len(self.capital_history) == self.t)\n",
    "\n",
    "        self.capital_history.append(self.capital)\n",
    "\n",
    "        # update state and determine reward\n",
    "\n",
    "        if len(self.capital_history) - self.lag < 0:\n",
    "            Y = 0\n",
    "        else:\n",
    "            if SHOCKS:\n",
    "                if SHOCK_TYPE == 'PERSISTENT'\n",
    "                    self.shock = np.random.uniform(0, 2)\n",
    "                    Y = self.shock * (self.capital_history[self.t - self.lag] ** ALPHA) * ((LABOR) ** (1 - ALPHA))\n",
    "                elif SHOCK_TYPE == 'INTERMITTENT'\n",
    "                    if len(self.capital_history) - self.lag < 0:\n",
    "                        Y = 0\n",
    "                    else:\n",
    "                        if SHOCKS:\n",
    "                            if env.t % 100 == 0:\n",
    "                                self.shock = np.random.uniform(0, 2)\n",
    "                            if env.t % 100 == 5:\n",
    "                                self.shock = 1\n",
    "            Y = self.shock * (self.capital_history[self.t - self.lag] ** ALPHA) * ((LABOR) ** (1 - ALPHA))\n",
    "\n",
    "        self.capital = self.capital + action * Y - DELTA * self.capital\n",
    "        reward = (1 - action) * Y\n",
    "        self.t += 1\n",
    "                \n",
    "        # determine terminating condition\n",
    "        if self.t >= EPISODE_LENGTH:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "            \n",
    "        info = {'production': Y,'shock': self.shock}\n",
    "        \n",
    "        # assert(self.capital.shape == (1,))\n",
    "        \n",
    "        if (type(reward) != float):\n",
    "            if type(reward) == np.ndarray and len(reward) == 1:\n",
    "                reward = reward[0]\n",
    "            if type(reward) == np.float32:\n",
    "                reward = reward.item()\n",
    "\n",
    "        assert(type(reward) == float)\n",
    "\n",
    "        # return step information\n",
    "        return self.capital, reward, done, info\n",
    "    \n",
    "    def reset(self, seed=None, return_info=False, **kwargs):\n",
    "        if seed:\n",
    "            super().reset(seed=seed)\n",
    "        \n",
    "        self.capital = STARTING_BUDGET\n",
    "        self.capital_history = []\n",
    "        self.t = 0\n",
    "        \n",
    "        info = {}\n",
    "\n",
    "        if return_info:\n",
    "            return (self.capital, info)\n",
    "        else:\n",
    "            return self.capital\n",
    "    \n",
    "    def render(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "431fjUn6Bklb"
   },
   "outputs": [],
   "source": [
    "env = CustomEnv(LAG)\n",
    "# check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--rBqUExFi95"
   },
   "outputs": [],
   "source": [
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJY-6uoWa8AI"
   },
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9Rb0Heza9Tl"
   },
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=BUFFER_CAPACITY, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        with tf.device(device_name):\n",
    "            # Training and updating Actor & Critic networks.\n",
    "            # See Pseudo Code.\n",
    "            with tf.GradientTape() as tape:\n",
    "                target_actions = target_actor(next_state_batch, training=True)\n",
    "                y = reward_batch + gamma * target_critic(\n",
    "                    [next_state_batch, target_actions], training=True\n",
    "                )\n",
    "                critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "                critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "            critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "            critic_optimizer.apply_gradients(\n",
    "                zip(critic_grad, critic_model.trainable_variables)\n",
    "            )\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                actions = actor_model(state_batch, training=True)\n",
    "                critic_value = critic_model([state_batch, actions], training=True)\n",
    "                # Used `-value` as we want to maximize the value given\n",
    "                # by the critic for our actions\n",
    "                actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "            actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "            actor_optimizer.apply_gradients(\n",
    "                zip(actor_grad, actor_model.trainable_variables)\n",
    "            )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMt0TFLsa-aA"
   },
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    with tf.device(device_name):\n",
    "        last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "        inputs = layers.Input(shape=(num_states,))\n",
    "        layer1 = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "        outputs = layers.Dense(1, activation=\"sigmoid\", kernel_initializer=last_init)(layer1)\n",
    "\n",
    "        # Scaling output to fit between 0 and 1\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        model.compile()\n",
    "        return model\n",
    "\n",
    "\n",
    "def get_critic():\n",
    "    with tf.device(device_name):\n",
    "        # State as input\n",
    "        state_input = layers.Input(shape=(num_states))\n",
    "        state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "\n",
    "        # Action as input\n",
    "        action_input = layers.Input(shape=(num_actions))\n",
    "        action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "        # Both are passed through seperate layer before concatenating\n",
    "        concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "        out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "        outputs = layers.Dense(1)(out)\n",
    "\n",
    "        # Outputs single value for give state-action\n",
    "        model = tf.keras.Model([state_input, action_input], outputs)\n",
    "        model.compile()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhEVyjxbbHQZ"
   },
   "outputs": [],
   "source": [
    "# def policy(state, noise_object, debugging=False):\n",
    "def policy(state, noise, debugging=False):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    if debugging:\n",
    "        print(\"sampled_actions: {}\".format(sampled_actions))\n",
    "        print(\"np.squeeze(legal_action): {}\".format(np.squeeze(legal_action)))\n",
    "\n",
    "    return np.squeeze(legal_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8_Dxnf9td4n"
   },
   "outputs": [],
   "source": [
    "# Actor-Critic Hyperparameters\n",
    "std_dev = 0.005\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.003\n",
    "actor_lr = 0.0003\n",
    "\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1IZT38c8KOd"
   },
   "outputs": [],
   "source": [
    "num_trials = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVV4Qn70bJYf",
    "outputId": "54312e17-7516-464b-e835-52901f375233"
   },
   "outputs": [],
   "source": [
    "columns = ['trial', 'round', 'lag', 'savings', 'capital', 'consumption', 'noise', 'shock', 'production']\n",
    "lags = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Used to update target networks\n",
    "tau = 0.01\n",
    "\n",
    "for local_lag in lags:\n",
    "    env = CustomEnv(local_lag)\n",
    "    for trial in range(num_trials):\n",
    "        trained_data = pd.DataFrame(columns = columns) # data from the progression of the last episode\n",
    "\n",
    "        # Reinitialize model\n",
    "\n",
    "        ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "        actor_model = get_actor()\n",
    "        critic_model = get_critic()\n",
    "\n",
    "        target_actor = get_actor()\n",
    "        target_critic = get_critic()\n",
    "\n",
    "        # Making the weights equal initially\n",
    "        target_actor.set_weights(actor_model.get_weights())\n",
    "        target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "        critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "        actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "        buffer = Buffer(BUFFER_CAPACITY, 64)\n",
    "\n",
    "        prev_state = env.reset()\n",
    "\n",
    "        while True:\n",
    "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "            debugging = False\n",
    "\n",
    "            if env.t < 500:\n",
    "                action = random.uniform(0.0, 1.0)\n",
    "                noise = 0\n",
    "            else:\n",
    "                noise = ou_noise()\n",
    "                action = policy(tf_prev_state, noise, debugging=debugging)\n",
    "\n",
    "            # Receive state and reward from environment.\n",
    "            state, reward, done, info = env.step(action)\n",
    "\n",
    "            trained_data.loc[len(trained_data.index)] = [trial, env.t, local_lag, action, state, reward, noise, info['shock'], info['production']]\n",
    "\n",
    "            buffer.record((prev_state, action, reward, state))\n",
    "\n",
    "            buffer.learn()\n",
    "            update_target(target_actor.variables, actor_model.variables, tau)\n",
    "            update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "            if env.t % 1000 == 0:\n",
    "                print(\"Round {} complete\".format(env.t))\n",
    "\n",
    "            # End this episode when `done` is True\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            prev_state = state"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
